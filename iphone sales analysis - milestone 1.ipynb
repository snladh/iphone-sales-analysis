{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cc719e6-1ec2-4573-8c43-c50514d7f483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# iphone Sales Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bba64be-df46-415d-847f-a736a8ef334b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Milestone 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa60160-f4da-4836-99bf-6f1f313a5af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"iphone_sales_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a50120b-a6c6-4a62-95c8-0c7cfa8c9d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# reading product_data csv\n",
    "\n",
    "product_filepath_csv = \"dbfs:/FileStore/shared_uploads/snl.adh97@gmail.com/product_data-1.csv\"\n",
    "\n",
    "# reading sales_data with delimiter '|'\n",
    "\n",
    "sales_filepath = \"dbfs:/FileStore/shared_uploads/snl.adh97@gmail.com/sales_data_pipe_delimited-1.csv\"\n",
    "\n",
    "# Sales data with 10,000 rows and Product data was created using chatGPT so that I can work with larger data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12c2b2c-3611-4aba-9f90-434ca7b00e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### API for Sales Data Collector\n",
    "\n",
    "##### It reads a sales data that has â”‚ has delimiter and has a header, and returns a partitioned hive-table in Parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c5df93-35fe-4e2c-abf6-db2f488d05fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sales Data API\n",
    "\n",
    "def sales_data_api(spark, text_file_path):\n",
    "    \n",
    "    # defining schema in order to keep control over data types\n",
    "    sales_schema = StructType([\n",
    "        StructField(\"seller_id\", IntegerType(), True),\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"buyer_id\", IntegerType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True),  # Read as string initially\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"price\", IntegerType(), True)])\n",
    "\n",
    "    # reading sales data in csv format\n",
    "    sales = spark.read.format(\"csv\").option(\"delimiter\",\"|\").schema(sales_schema).option(\"header\",True).load(sales_filepath)\n",
    "\n",
    "    # converting sale_date column to datetype from stringtype\n",
    "    sales = sales.withColumn(\"sale_date\", col(\"sale_date\").cast(DateType()))\n",
    "\n",
    "    # creating a hive table with partitioned sale_date\n",
    "    hive_table_name = \"partitioned_sales_date\"\n",
    "\n",
    "    sales.write.mode(\"overwrite\").partitionBy(\"sale_date\").format(\"parquet\").saveAsTable(hive_table_name)\n",
    "    print(f\"Hive Table with name '{hive_table_name}' created sucessfully.\")\n",
    "\n",
    "    return hive_table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6547ad4-9dad-41bc-a13f-a317c320df86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Product Data Collector API\n",
    "\n",
    "#### It reads product data from a Parquet file and writes it into a non-partitioned Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5426f0-525d-4a50-8e72-905471ca8c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def product_data_api(spark, file_path):\n",
    "\n",
    "    # defining schema in order to keep control over the data\n",
    "    product_schema = StructType ([\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"unit_price\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    # reading product data in csv format\n",
    "    product = spark.read.format(\"csv\").option(\"header\",True).schema(product_schema).load(file_path)\n",
    "\n",
    "    # defining the Hive table name\n",
    "    hive_table_name = \"products_table\"\n",
    "\n",
    "    # writing the table in non-partitioned parquet format Hive Table\n",
    "    product.write.mode(\"overwrite\").format(\"parquet\").saveAsTable(hive_table_name)\n",
    "\n",
    "    print(f\"Hive table with name '{hive_table_name}' created successfully.\")\n",
    "    return hive_table_name"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "iphone sales analysis - milestone 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
