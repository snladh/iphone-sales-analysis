{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "317e8231-afe4-4a8b-9624-db304ecc5559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# iphone Sales Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858f1dfd-38f8-4d8d-b5e0-93ce05c61301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572bf521-f9d7-4938-800c-50369db854f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"iphone_sales_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a34fdc22-2fa0-4f98-92af-218523c1378d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### API for Sales Data Collector\n",
    "\n",
    "##### It reads a sales data that has â”‚ has delimiter and has a header, and returns a partitioned hive-table in Parquet format.\n",
    "\n",
    "##### Sales data with 10,000 rows and Product data was created using chatGPT so that I can work with larger data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203f90d2-cf09-41d8-a5f6-47b2587f674e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_filepath_csv = \"dbfs:/FileStore/shared_uploads/snl.adh97@gmail.com/sales_data.csv\"\n",
    "\n",
    "product_filepath_csv = \"dbfs:/FileStore/shared_uploads/snl.adh97@gmail.com/product_data.csv\"\n",
    "\n",
    "sales_filepath = \"dbfs:/FileStore/shared_uploads/snl.adh97@gmail.com/large_sales_data_pipe_delimited.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccfcac8-69ce-44b6-8394-4d422c576dcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def sales_data_api(spark, text_file_path):\n",
    "    \n",
    "    # defining schema in order to keep control over data types\n",
    "    sales_schema = StructType([\n",
    "        StructField(\"seller_id\", IntegerType(), True),\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"buyer_id\", IntegerType(), True),\n",
    "        StructField(\"sale_date\", StringType(), True),  # Read as string initially\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"price\", IntegerType(), True)])\n",
    "\n",
    "    # reading sales data in csv format\n",
    "    sales = spark.read.format(\"csv\").option(\"delimiter\",\"|\").schema(sales_schema).option(\"header\",True).load(sales_filepath)\n",
    "\n",
    "    # converting sale_date column to datetype from stringtype\n",
    "    sales = sales.withColumn(\"sale_date\", col(\"sale_date\").cast(DateType()))\n",
    "\n",
    "    # creating a hive table with partitioned sale_date\n",
    "    hive_table_name = \"sale_date_partitioned\"\n",
    "\n",
    "    sales.write.mode(\"overwrite\").partitionBy(\"sale_date\").format(\"parquet\").saveAsTable(hive_table_name)\n",
    "\n",
    "    return hive_table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8db65b3-6e29-433f-9f7e-8e5f581f04fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hive table 'sales_partitioned' contains 10000 rows.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the table\n",
    "# row_count = spark.sql(f\"SELECT COUNT(*) AS row_count FROM {hive_table_name}\").collect()[0][\"row_count\"]\n",
    "# print(f\"The Hive table '{hive_table_name}' contains {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eab76cd-644d-4648-9f3e-c92b679f2a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: 'sale_date_partitioned'"
     ]
    }
   ],
   "source": [
    "sales_data_api(spark,sales_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34ab05a0-ec51-49c5-a49e-eb3ecf736dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "iphone sales analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
